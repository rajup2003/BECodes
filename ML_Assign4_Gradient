# -*- coding: utf-8 -*-
"""Assignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tkBZu-ls3w1Kc6psya7f49id7R9NHqKt
"""

# Define the function and its derivative
def function(x):
    return (x + 3) ** 2

def derivative(x):
    return 2 * (x + 3)

# Gradient Descent function to find the local minima
def gradient_descent(learning_rate, max_iterations, initial_x):
    x = initial_x
    for _ in range(max_iterations):
        gradient = derivative(x)
        x = x - learning_rate * gradient
    return x, function(x)
//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
# Set the initial parameters
learning_rate = 0.1  # Learning rate for Gradient Descent
max_iterations = 1000  # Maximum number of iterations
initial_x = 2  # Initial value of x

# Run Gradient Descent algorithm
minima_x, minima_y = gradient_descent(learning_rate, max_iterations, initial_x)

# Output the results
print("Local Minima Found at x =", minima_x)
print("Minimum Value of y at the Local Minima =", minima_y)

x=2  # Starting Point
lr=0.01  # Learning Rate
precision=0.00001  # Upto what decimal point or upto what precision
previous_step_size=1
max_iterator=1000 # how many iteration you want
iters=1
gf=lambda x: (x+3)**2
gd=[]
while precision <previous_step_size and iters <max_iterator:
    prev=x
    x=x- lr * gf(prev)
    previous_step_size=abs(x-prev)
    iters=iters+1
    print(f"Iterators:{iters} value:{x}")
    gd.append(x)
print("Local minima",x)

import matplotlib.pyplot as plt
plt.plot(gd)
